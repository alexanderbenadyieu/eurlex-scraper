{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EurLex Scraper Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prom_value(line):\n",
    "    \"\"\"Parse a Prometheus metrics line.\"\"\"\n",
    "    # Skip help and type lines\n",
    "    if line.startswith('#'):\n",
    "        return None, None\n",
    "    \n",
    "    # Matches formats like:\n",
    "    # metric_name{label1=\"value1\",label2=\"value2\"} value\n",
    "    # or\n",
    "    # metric_name value\n",
    "    match = re.match(r'^(\\w+)(?:\\{[^}]*\\})?\\s+(\\d+(\\.\\d+)?)', line)\n",
    "    if match:\n",
    "        metric = match.group(1)\n",
    "        value = float(match.group(2))\n",
    "        return metric, value\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prom_file(file):\n",
    "    \"\"\"Parse a Prometheus metrics file and extract key metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'timestamp': datetime.fromtimestamp(0),  # Default timestamp\n",
    "        'documents_processed_total': 0,\n",
    "        'requests_total': 0,\n",
    "        'requests_success': 0,\n",
    "        'requests_failure': 0,\n",
    "        'retry_attempts_total': 0,\n",
    "        'validation_errors_total': 0,\n",
    "        'storage_size_bytes': 0\n",
    "    }\n",
    "    \n",
    "    # Try to extract timestamp from filename\n",
    "    try:\n",
    "        timestamp_str = file.stem.split('_')[1] + '_' + file.stem.split('_')[2]\n",
    "        metrics['timestamp'] = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            metric, value = parse_prom_value(line)\n",
    "            \n",
    "            if metric:\n",
    "                # Normalize metric names\n",
    "                normalized_metric = metric.lower()\n",
    "                \n",
    "                # Map metrics to our keys\n",
    "                if 'documents_processed_total' in normalized_metric:\n",
    "                    metrics['documents_processed_total'] = value\n",
    "                elif 'requests_total' in normalized_metric:\n",
    "                    metrics['requests_total'] = value\n",
    "                elif 'retry_attempts_total' in normalized_metric:\n",
    "                    metrics['retry_attempts_total'] = value\n",
    "                elif 'validation_errors_total' in normalized_metric:\n",
    "                    metrics['validation_errors_total'] = value\n",
    "                elif 'storage_size_bytes' in normalized_metric:\n",
    "                    metrics['storage_size_bytes'] = value\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from .prom files\n",
    "metrics_dir = Path('../metrics')\n",
    "metrics_files = sorted(list(metrics_dir.glob('*.prom')))\n",
    "\n",
    "# Process metrics files\n",
    "metrics_data = []\n",
    "for file in metrics_files:\n",
    "    try:\n",
    "        metrics = parse_prom_file(file)\n",
    "        metrics_data.append(metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Filter out sessions with 0 or 1 documents processed\n",
    "df_filtered = df[df['documents_processed_total'] > 1]\n",
    "\n",
    "# Sort by timestamp\n",
    "df_filtered = df_filtered.sort_values('timestamp')\n",
    "\n",
    "# Print summary\n",
    "print(\"Total number of metrics files:\", len(df))\n",
    "print(\"Number of metrics files after filtering:\", len(df_filtered))\n",
    "print(\"\\nMetrics summary (filtered):\")\n",
    "print(df_filtered.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df_filtered['timestamp'], df_filtered['documents_processed_total'], label='Documents Processed')\n",
    "plt.title('Documents Processed')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df_filtered['timestamp'], df_filtered['requests_total'], label='Total Requests')\n",
    "plt.title('Total Requests')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df_filtered['timestamp'], df_filtered['retry_attempts_total'], label='Retry Attempts')\n",
    "plt.title('Retry Attempts')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df_filtered['timestamp'], df_filtered['storage_size_bytes'], label='Storage Size')\n",
    "plt.title('Storage Size')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Bytes')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/metrics_analysis.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
